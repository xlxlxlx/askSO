<code>for i in range(100):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code>
<code>train_step</code>
<code>train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
</code>
<code>cross_entropy</code>
<code>cross_entropy = -tf.reduce_sum(y_ * tf.log(y))
</code>
<code>cross_entropy</code>
<code>for i in range(100):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    cross_entropy = -tf.reduce_sum(y_ * tf.log(y))
    print 'loss = ' + str(cross_entropy)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code>
<code>cross_entropy</code>
<code>sess.run(train_step, ...)</code>
<code>cross_entropy</code>
<code>sess.run(train_step, ...)</code>
<code>tf.Variable</code>
<code>str(cross_entropy)</code>